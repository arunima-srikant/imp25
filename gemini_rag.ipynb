{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f1d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\aruni\\onedrive\\desktop\\sip-ii\\.venv\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba07576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai.types import Content, GenerationConfig, GenerateContentConfig # <-- Import the parent config class\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f1b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Client initialized successfully using key from .env file.\n"
     ]
    }
   ],
   "source": [
    "#1. Setup\n",
    "# Assumes GEMINI_API_KEY is set in your environment\n",
    "\n",
    "# Load variables from the .env file in the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# The client automatically finds and uses the key\n",
    "client = genai.Client()\n",
    "print(\"Gemini Client initialized successfully using key from .env file.\")\n",
    "\n",
    "MODEL_NAME = 'gemini-2.5-flash'\n",
    "CONTEXT_FILE_PATH = \"fat_lab.json\" # <-- Using the JSON file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ed4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Load and Parse the JSON Context\n",
    "def load_and_parse_json(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file containing a list of Q&A entries and converts it \n",
    "    into a single, structured text block for the LLM context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Context file not found at '{file_path}'.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{file_path}'. Ensure the file is valid JSON.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "    context_parts = []\n",
    "    \n",
    "    # Iterate through each Q&A object in the JSON list\n",
    "    for entry in data:\n",
    "        speaker = entry.get(\"speaker\", \"Unknown\")\n",
    "        topic = entry.get(\"topic\", \"General\")\n",
    "        question = entry.get(\"question\", \"\")\n",
    "        answer = entry.get(\"answer\", \"\")\n",
    "        tags = \", \".join(entry.get(\"tags\", []))\n",
    "        \n",
    "        # Format the entry into a single text block\n",
    "        chunk = (\n",
    "            f\"Speaker: {speaker}\\n\"\n",
    "            f\"Topic: {topic}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Answer: {answer}\\n\"\n",
    "            f\"Tags: {tags}\\n\"\n",
    "        )\n",
    "        context_parts.append(chunk)\n",
    "\n",
    "    # Join all formatted chunks with two newlines for separation\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "full_interview_context = load_and_parse_json(CONTEXT_FILE_PATH)\n",
    "\n",
    "if not full_interview_context:\n",
    "    exit() # Stop execution if context couldn't be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa33d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Construct the Prompt\n",
    "SYSTEM_INSTRUCTION = f\"\"\"\n",
    "You are a useful chatbot, built to answer questions that cafe entreprenuers when starting a new cafe.\n",
    "Use the following interview transcript context to answer the user's request.\n",
    "If the answer is not contained in the context, say you don't know.\n",
    "\n",
    "--- INTERVIEW CONTEXT START ---\n",
    "{full_interview_context}\n",
    "--- INTERVIEW CONTEXT END ---\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74587f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. User Query\n",
    "user_query = \"What percentage of the budget is allocated to marketing, and what was the gap they noticed in the market?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21de8469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What percentage of the budget is allocated to marketing, and what was the gap they noticed in the market?\n",
      "\n",
      "Loading context from: fat_lab.json (Length: 7581 characters)\n",
      "--- Generating response with Gemini ---\n",
      "\n",
      "Chatbot Response:\n",
      "Around **1.5–2% of their revenue** is allocated to direct marketing and agencies.\n",
      "\n",
      "The market gap they noticed was a **lack of cafes serving good filter coffee and omelettes in Pune**. This niche inspired their initial limited menu.\n",
      "\n",
      "Chatbot Response:\n",
      "Around **1.5–2% of their revenue** is allocated to direct marketing and agencies.\n",
      "\n",
      "The market gap they noticed was a **lack of cafes serving good filter coffee and omelettes in Pune**. This niche inspired their initial limited menu.\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate Content\n",
    "\n",
    "# Prepare contents list\n",
    "contents = [\n",
    "    # Message 1: The System Instruction + Full Context\n",
    "    Content(role='user', parts=[genai.types.Part.from_text(text=SYSTEM_INSTRUCTION)]),\n",
    "    # Message 2: The actual User Question\n",
    "    Content(role='user', parts=[genai.types.Part.from_text(text=user_query)])\n",
    "]\n",
    "\n",
    "# FIX: Combine GenerationConfig and GenerateContentConfig into one call.\n",
    "# This structure satisfies the requirement of the Pydantic validation \n",
    "# in your current SDK version by passing the parameter directly.\n",
    "full_config = GenerateContentConfig(\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(f\"User Query: {user_query}\\n\")\n",
    "print(f\"Loading context from: {CONTEXT_FILE_PATH} (Length: {len(full_interview_context)} characters)\")\n",
    "print(\"--- Generating response with Gemini ---\")\n",
    "\n",
    "# --- Final API Call ---\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_NAME,\n",
    "    contents=contents,\n",
    "    config=full_config \n",
    ")\n",
    "\n",
    "print(\"\\nChatbot Response:\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
